{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FYwXlr_yppwl",
        "outputId": "1e00603a-2b3f-483d-a04d-3c0337c48f18"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Archive:  /content/sp_colab.zip\n",
            "   creating: sp_colab/\n",
            "  inflating: __MACOSX/._sp_colab     \n",
            "  inflating: sp_colab/spell_correction_gtc.py  \n",
            "  inflating: __MACOSX/sp_colab/._spell_correction_gtc.py  \n",
            "  inflating: sp_colab/train_bart_model_gtc.sh  \n",
            "  inflating: __MACOSX/sp_colab/._train_bart_model_gtc.sh  \n",
            "  inflating: sp_colab/.DS_Store      \n",
            "  inflating: __MACOSX/sp_colab/._.DS_Store  \n",
            "  inflating: sp_colab/spell_correction_pretrained.py  \n",
            "  inflating: __MACOSX/sp_colab/._spell_correction_pretrained.py  \n",
            "  inflating: sp_colab/requirements.txt  \n",
            "  inflating: __MACOSX/sp_colab/._requirements.txt  \n",
            "  inflating: sp_colab/run_summarization.py  \n",
            "  inflating: __MACOSX/sp_colab/._run_summarization.py  \n",
            "   creating: sp_colab/models/\n",
            "  inflating: __MACOSX/sp_colab/._models  \n",
            "   creating: sp_colab/__pycache__/\n",
            "  inflating: __MACOSX/sp_colab/.___pycache__  \n",
            "  inflating: sp_colab/.gitignore     \n",
            "  inflating: __MACOSX/sp_colab/._.gitignore  \n",
            "   creating: sp_colab/data/\n",
            "  inflating: __MACOSX/sp_colab/._data  \n",
            "  inflating: sp_colab/generate_dataset_gtc.py  \n",
            "  inflating: __MACOSX/sp_colab/._generate_dataset_gtc.py  \n",
            "  inflating: sp_colab/models/.DS_Store  \n",
            "  inflating: __MACOSX/sp_colab/models/._.DS_Store  \n",
            "   creating: sp_colab/models/bart-base-gtc/\n",
            "  inflating: __MACOSX/sp_colab/models/._bart-base-gtc  \n",
            "  inflating: sp_colab/__pycache__/const.cpython-310.pyc  \n",
            "  inflating: __MACOSX/sp_colab/__pycache__/._const.cpython-310.pyc  \n",
            "  inflating: sp_colab/__pycache__/neuspell.cpython-310.pyc  \n",
            "  inflating: __MACOSX/sp_colab/__pycache__/._neuspell.cpython-310.pyc  \n",
            "  inflating: sp_colab/data/.DS_Store  \n",
            "  inflating: __MACOSX/sp_colab/data/._.DS_Store  \n",
            "  inflating: sp_colab/data/cor.txt   \n",
            "  inflating: __MACOSX/sp_colab/data/._cor.txt  \n",
            "  inflating: sp_colab/data/gtc-correct.txt  \n",
            "  inflating: __MACOSX/sp_colab/data/._gtc-correct.txt  \n",
            "  inflating: sp_colab/data/gtc-incorrect.txt  \n",
            "  inflating: __MACOSX/sp_colab/data/._gtc-incorrect.txt  \n",
            "  inflating: sp_colab/data/inc.txt   \n",
            "  inflating: __MACOSX/sp_colab/data/._inc.txt  \n",
            "  inflating: sp_colab/models/bart-base-gtc/.DS_Store  \n",
            "  inflating: __MACOSX/sp_colab/models/bart-base-gtc/._.DS_Store  \n",
            "   creating: sp_colab/models/bart-base-gtc/checkpoint-500/\n",
            "  inflating: __MACOSX/sp_colab/models/bart-base-gtc/._checkpoint-500  \n",
            "  inflating: sp_colab/models/bart-base-gtc/tokenizer_config.json  \n",
            "  inflating: __MACOSX/sp_colab/models/bart-base-gtc/._tokenizer_config.json  \n",
            "  inflating: sp_colab/models/bart-base-gtc/special_tokens_map.json  \n",
            "  inflating: __MACOSX/sp_colab/models/bart-base-gtc/._special_tokens_map.json  \n",
            "   creating: sp_colab/models/bart-base-gtc/checkpoint-1000/\n",
            "  inflating: __MACOSX/sp_colab/models/bart-base-gtc/._checkpoint-1000  \n",
            "  inflating: sp_colab/models/bart-base-gtc/config.json  \n",
            "  inflating: __MACOSX/sp_colab/models/bart-base-gtc/._config.json  \n",
            "  inflating: sp_colab/models/bart-base-gtc/tokenizer.json  \n",
            "  inflating: __MACOSX/sp_colab/models/bart-base-gtc/._tokenizer.json  \n",
            "  inflating: sp_colab/models/bart-base-gtc/train_results.json  \n",
            "  inflating: __MACOSX/sp_colab/models/bart-base-gtc/._train_results.json  \n",
            "  inflating: sp_colab/models/bart-base-gtc/generation_config.json  \n",
            "  inflating: __MACOSX/sp_colab/models/bart-base-gtc/._generation_config.json  \n",
            "  inflating: sp_colab/models/bart-base-gtc/merges.txt  \n",
            "  inflating: __MACOSX/sp_colab/models/bart-base-gtc/._merges.txt  \n",
            "  inflating: sp_colab/models/bart-base-gtc/training_args.bin  \n",
            "  inflating: __MACOSX/sp_colab/models/bart-base-gtc/._training_args.bin  \n",
            "  inflating: sp_colab/models/bart-base-gtc/pytorch_model.bin  \n",
            "  inflating: __MACOSX/sp_colab/models/bart-base-gtc/._pytorch_model.bin  \n",
            "  inflating: sp_colab/models/bart-base-gtc/vocab.json  \n",
            "  inflating: __MACOSX/sp_colab/models/bart-base-gtc/._vocab.json  \n",
            "  inflating: sp_colab/models/bart-base-gtc/all_results.json  \n",
            "  inflating: __MACOSX/sp_colab/models/bart-base-gtc/._all_results.json  \n",
            "   creating: sp_colab/models/bart-base-gtc/runs/\n",
            "  inflating: __MACOSX/sp_colab/models/bart-base-gtc/._runs  \n",
            "  inflating: sp_colab/models/bart-base-gtc/trainer_state.json  \n",
            "  inflating: __MACOSX/sp_colab/models/bart-base-gtc/._trainer_state.json  \n",
            "  inflating: sp_colab/models/bart-base-gtc/checkpoint-500/rng_state.pth  \n",
            "  inflating: __MACOSX/sp_colab/models/bart-base-gtc/checkpoint-500/._rng_state.pth  \n",
            "  inflating: sp_colab/models/bart-base-gtc/checkpoint-500/tokenizer_config.json  \n",
            "  inflating: __MACOSX/sp_colab/models/bart-base-gtc/checkpoint-500/._tokenizer_config.json  \n",
            "  inflating: sp_colab/models/bart-base-gtc/checkpoint-500/special_tokens_map.json  \n",
            "  inflating: __MACOSX/sp_colab/models/bart-base-gtc/checkpoint-500/._special_tokens_map.json  \n",
            "  inflating: sp_colab/models/bart-base-gtc/checkpoint-500/optimizer.pt  \n",
            "  inflating: __MACOSX/sp_colab/models/bart-base-gtc/checkpoint-500/._optimizer.pt  \n",
            "  inflating: sp_colab/models/bart-base-gtc/checkpoint-500/config.json  \n",
            "  inflating: __MACOSX/sp_colab/models/bart-base-gtc/checkpoint-500/._config.json  \n",
            "  inflating: sp_colab/models/bart-base-gtc/checkpoint-500/scheduler.pt  \n",
            "  inflating: __MACOSX/sp_colab/models/bart-base-gtc/checkpoint-500/._scheduler.pt  \n",
            "  inflating: sp_colab/models/bart-base-gtc/checkpoint-500/tokenizer.json  \n",
            "  inflating: __MACOSX/sp_colab/models/bart-base-gtc/checkpoint-500/._tokenizer.json  \n",
            "  inflating: sp_colab/models/bart-base-gtc/checkpoint-500/generation_config.json  \n",
            "  inflating: __MACOSX/sp_colab/models/bart-base-gtc/checkpoint-500/._generation_config.json  \n",
            "  inflating: sp_colab/models/bart-base-gtc/checkpoint-500/merges.txt  \n",
            "  inflating: __MACOSX/sp_colab/models/bart-base-gtc/checkpoint-500/._merges.txt  \n",
            "  inflating: sp_colab/models/bart-base-gtc/checkpoint-500/training_args.bin  \n",
            "  inflating: __MACOSX/sp_colab/models/bart-base-gtc/checkpoint-500/._training_args.bin  \n",
            "  inflating: sp_colab/models/bart-base-gtc/checkpoint-500/pytorch_model.bin  \n",
            "  inflating: __MACOSX/sp_colab/models/bart-base-gtc/checkpoint-500/._pytorch_model.bin  \n",
            "  inflating: sp_colab/models/bart-base-gtc/checkpoint-500/vocab.json  \n",
            "  inflating: __MACOSX/sp_colab/models/bart-base-gtc/checkpoint-500/._vocab.json  \n",
            "  inflating: sp_colab/models/bart-base-gtc/checkpoint-500/scaler.pt  \n",
            "  inflating: __MACOSX/sp_colab/models/bart-base-gtc/checkpoint-500/._scaler.pt  \n",
            "  inflating: sp_colab/models/bart-base-gtc/checkpoint-500/trainer_state.json  \n",
            "  inflating: __MACOSX/sp_colab/models/bart-base-gtc/checkpoint-500/._trainer_state.json  \n",
            "  inflating: sp_colab/models/bart-base-gtc/checkpoint-1000/rng_state.pth  \n",
            "  inflating: __MACOSX/sp_colab/models/bart-base-gtc/checkpoint-1000/._rng_state.pth  \n",
            "  inflating: sp_colab/models/bart-base-gtc/checkpoint-1000/tokenizer_config.json  \n",
            "  inflating: __MACOSX/sp_colab/models/bart-base-gtc/checkpoint-1000/._tokenizer_config.json  \n",
            "  inflating: sp_colab/models/bart-base-gtc/checkpoint-1000/special_tokens_map.json  \n",
            "  inflating: __MACOSX/sp_colab/models/bart-base-gtc/checkpoint-1000/._special_tokens_map.json  \n",
            "  inflating: sp_colab/models/bart-base-gtc/checkpoint-1000/optimizer.pt  \n",
            "  inflating: __MACOSX/sp_colab/models/bart-base-gtc/checkpoint-1000/._optimizer.pt  \n",
            "  inflating: sp_colab/models/bart-base-gtc/checkpoint-1000/config.json  \n",
            "  inflating: __MACOSX/sp_colab/models/bart-base-gtc/checkpoint-1000/._config.json  \n",
            "  inflating: sp_colab/models/bart-base-gtc/checkpoint-1000/scheduler.pt  \n",
            "  inflating: __MACOSX/sp_colab/models/bart-base-gtc/checkpoint-1000/._scheduler.pt  \n",
            "  inflating: sp_colab/models/bart-base-gtc/checkpoint-1000/tokenizer.json  \n",
            "  inflating: __MACOSX/sp_colab/models/bart-base-gtc/checkpoint-1000/._tokenizer.json  \n",
            "  inflating: sp_colab/models/bart-base-gtc/checkpoint-1000/generation_config.json  \n",
            "  inflating: __MACOSX/sp_colab/models/bart-base-gtc/checkpoint-1000/._generation_config.json  \n",
            "  inflating: sp_colab/models/bart-base-gtc/checkpoint-1000/merges.txt  \n",
            "  inflating: __MACOSX/sp_colab/models/bart-base-gtc/checkpoint-1000/._merges.txt  \n",
            "  inflating: sp_colab/models/bart-base-gtc/checkpoint-1000/training_args.bin  \n",
            "  inflating: __MACOSX/sp_colab/models/bart-base-gtc/checkpoint-1000/._training_args.bin  \n",
            "  inflating: sp_colab/models/bart-base-gtc/checkpoint-1000/pytorch_model.bin  \n",
            "  inflating: __MACOSX/sp_colab/models/bart-base-gtc/checkpoint-1000/._pytorch_model.bin  \n",
            "  inflating: sp_colab/models/bart-base-gtc/checkpoint-1000/vocab.json  \n",
            "  inflating: __MACOSX/sp_colab/models/bart-base-gtc/checkpoint-1000/._vocab.json  \n",
            "  inflating: sp_colab/models/bart-base-gtc/checkpoint-1000/scaler.pt  \n",
            "  inflating: __MACOSX/sp_colab/models/bart-base-gtc/checkpoint-1000/._scaler.pt  \n",
            "  inflating: sp_colab/models/bart-base-gtc/checkpoint-1000/trainer_state.json  \n",
            "  inflating: __MACOSX/sp_colab/models/bart-base-gtc/checkpoint-1000/._trainer_state.json  \n",
            "   creating: sp_colab/models/bart-base-gtc/runs/Apr16_23-51-57_82874904c2c1/\n",
            "  inflating: __MACOSX/sp_colab/models/bart-base-gtc/runs/._Apr16_23-51-57_82874904c2c1  \n",
            "   creating: sp_colab/models/bart-base-gtc/runs/Apr16_23-51-57_82874904c2c1/1681689227.6885097/\n",
            "  inflating: __MACOSX/sp_colab/models/bart-base-gtc/runs/Apr16_23-51-57_82874904c2c1/._1681689227.6885097  \n",
            "  inflating: sp_colab/models/bart-base-gtc/runs/Apr16_23-51-57_82874904c2c1/events.out.tfevents.1681689227.82874904c2c1.6170.0  \n",
            "  inflating: __MACOSX/sp_colab/models/bart-base-gtc/runs/Apr16_23-51-57_82874904c2c1/._events.out.tfevents.1681689227.82874904c2c1.6170.0  \n",
            "  inflating: sp_colab/models/bart-base-gtc/runs/Apr16_23-51-57_82874904c2c1/1681689227.6885097/events.out.tfevents.1681689227.82874904c2c1.6170.1  \n",
            "  inflating: __MACOSX/sp_colab/models/bart-base-gtc/runs/Apr16_23-51-57_82874904c2c1/1681689227.6885097/._events.out.tfevents.1681689227.82874904c2c1.6170.1  \n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "!cp '/content/gdrive/MyDrive/gtc_pred/sp_colab.zip' '/content/'\n",
        "!unzip /content/sp_colab.zip\n",
        "!rm -rf /content/sp_colab.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ep0IQX_uCmY7",
        "outputId": "cc638584-353a-42fd-d049-473c2e68a54f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  /content/sp_colab.zip\n",
            "   creating: sp_colab/\n",
            "  inflating: __MACOSX/._sp_colab     \n",
            "  inflating: sp_colab/spell_correction_gtc.py  \n",
            "  inflating: __MACOSX/sp_colab/._spell_correction_gtc.py  \n",
            "  inflating: sp_colab/train_bart_model_gtc.sh  \n",
            "  inflating: __MACOSX/sp_colab/._train_bart_model_gtc.sh  \n",
            "  inflating: sp_colab/.DS_Store      \n",
            "  inflating: __MACOSX/sp_colab/._.DS_Store  \n",
            "  inflating: sp_colab/spell_correction_pretrained.py  \n",
            "  inflating: __MACOSX/sp_colab/._spell_correction_pretrained.py  \n",
            "  inflating: sp_colab/requirements.txt  \n",
            "  inflating: __MACOSX/sp_colab/._requirements.txt  \n",
            "  inflating: sp_colab/run_summarization.py  \n",
            "  inflating: __MACOSX/sp_colab/._run_summarization.py  \n",
            "   creating: sp_colab/__pycache__/\n",
            "  inflating: __MACOSX/sp_colab/.___pycache__  \n",
            "  inflating: sp_colab/.gitignore     \n",
            "  inflating: __MACOSX/sp_colab/._.gitignore  \n",
            "   creating: sp_colab/data/\n",
            "  inflating: __MACOSX/sp_colab/._data  \n",
            "  inflating: sp_colab/generate_dataset_gtc.py  \n",
            "  inflating: __MACOSX/sp_colab/._generate_dataset_gtc.py  \n",
            "  inflating: sp_colab/__pycache__/const.cpython-310.pyc  \n",
            "  inflating: __MACOSX/sp_colab/__pycache__/._const.cpython-310.pyc  \n",
            "  inflating: sp_colab/__pycache__/neuspell.cpython-310.pyc  \n",
            "  inflating: __MACOSX/sp_colab/__pycache__/._neuspell.cpython-310.pyc  \n",
            "  inflating: sp_colab/data/.DS_Store  \n",
            "  inflating: __MACOSX/sp_colab/data/._.DS_Store  \n",
            "  inflating: sp_colab/data/cor.txt   \n",
            "  inflating: __MACOSX/sp_colab/data/._cor.txt  \n",
            "  inflating: sp_colab/data/gtc-correct.txt  \n",
            "  inflating: __MACOSX/sp_colab/data/._gtc-correct.txt  \n",
            "  inflating: sp_colab/data/gtc-incorrect.txt  \n",
            "  inflating: __MACOSX/sp_colab/data/._gtc-incorrect.txt  \n",
            "  inflating: sp_colab/data/inc.txt   \n",
            "  inflating: __MACOSX/sp_colab/data/._inc.txt  \n"
          ]
        }
      ],
      "source": [
        "!unzip /content/sp_colab.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUlGiFhSdjmA"
      },
      "source": [
        "## clean-up"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Mxw9hYILCrLs"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "shutil.rmtree(\"/content/__MACOSX\", ignore_errors=True)\n",
        "shutil.rmtree(\"/content/sample_data\", ignore_errors=True)\n",
        "shutil.rmtree(\"/content/sp_colab.zip\", ignore_errors=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAjXPq09d0g5"
      },
      "source": [
        "## installing requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yhCZ6cumDZC3",
        "outputId": "285d24f1-a0e1-4cb1-caf5-233637822a7f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m89.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pytorch-lightning\n",
            "  Downloading pytorch_lightning-2.0.1.post0-py3-none-any.whl (718 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m718.6/718.6 kB\u001b[0m \u001b[31m68.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets>=1.8.0\n",
            "  Downloading datasets-2.11.0-py3-none-any.whl (468 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.7/468.7 kB\u001b[0m \u001b[31m54.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentencepiece!=0.1.92\n",
            "  Downloading sentencepiece-0.1.98-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m81.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.9/dist-packages (from -r /content/sp_colab/requirements.txt (line 5)) (3.20.3)\n",
            "Collecting rouge-score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.9/dist-packages (from -r /content/sp_colab/requirements.txt (line 7)) (3.8.1)\n",
            "Collecting py7zr\n",
            "  Downloading py7zr-0.20.5-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.3 in /usr/local/lib/python3.9/dist-packages (from -r /content/sp_colab/requirements.txt (line 9)) (2.0.0+cu118)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.9/dist-packages (from -r /content/sp_colab/requirements.txt (line 10)) (0.40.0)\n",
            "Collecting jiwer\n",
            "  Downloading jiwer-3.0.1-py3-none-any.whl (21 kB)\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m101.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers->-r /content/sp_colab/requirements.txt (line 1)) (23.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers->-r /content/sp_colab/requirements.txt (line 1)) (2.27.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers->-r /content/sp_colab/requirements.txt (line 1)) (3.11.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers->-r /content/sp_colab/requirements.txt (line 1)) (4.65.0)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.13.4-py3-none-any.whl (200 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.1/200.1 kB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers->-r /content/sp_colab/requirements.txt (line 1)) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers->-r /content/sp_colab/requirements.txt (line 1)) (1.22.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers->-r /content/sp_colab/requirements.txt (line 1)) (2022.10.31)\n",
            "Requirement already satisfied: fsspec[http]>2021.06.0 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning->-r /content/sp_colab/requirements.txt (line 2)) (2023.4.0)\n",
            "Collecting torchmetrics>=0.7.0\n",
            "  Downloading torchmetrics-0.11.4-py3-none-any.whl (519 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.2/519.2 kB\u001b[0m \u001b[31m54.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.9/dist-packages (from pytorch-lightning->-r /content/sp_colab/requirements.txt (line 2)) (4.5.0)\n",
            "Collecting lightning-utilities>=0.7.0\n",
            "  Downloading lightning_utilities-0.8.0-py3-none-any.whl (20 kB)\n",
            "Collecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets>=1.8.0->-r /content/sp_colab/requirements.txt (line 3)) (9.0.0)\n",
            "Collecting multiprocess\n",
            "  Downloading multiprocess-0.70.14-py39-none-any.whl (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.9/132.9 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiohttp\n",
            "  Downloading aiohttp-3.8.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m78.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dill<0.3.7,>=0.3.0\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting xxhash\n",
            "  Downloading xxhash-3.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.2/212.2 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from datasets>=1.8.0->-r /content/sp_colab/requirements.txt (line 3)) (1.5.3)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.9/dist-packages (from rouge-score->-r /content/sp_colab/requirements.txt (line 6)) (1.4.0)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.9/dist-packages (from rouge-score->-r /content/sp_colab/requirements.txt (line 6)) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nltk->-r /content/sp_colab/requirements.txt (line 7)) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from nltk->-r /content/sp_colab/requirements.txt (line 7)) (1.2.0)\n",
            "Collecting pybcj>=0.6.0\n",
            "  Downloading pybcj-1.0.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting inflate64>=0.3.1\n",
            "  Downloading inflate64-0.3.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (92 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.0/93.0 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.9/dist-packages (from py7zr->-r /content/sp_colab/requirements.txt (line 8)) (5.9.5)\n",
            "Collecting brotli>=1.0.9\n",
            "  Downloading Brotli-1.0.9-cp39-cp39-manylinux1_x86_64.whl (357 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m357.2/357.2 kB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting texttable\n",
            "  Downloading texttable-1.6.7-py2.py3-none-any.whl (10 kB)\n",
            "Collecting pycryptodomex>=3.6.6\n",
            "  Downloading pycryptodomex-3.17-cp35-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m90.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multivolumefile>=0.2.3\n",
            "  Downloading multivolumefile-0.2.3-py3-none-any.whl (17 kB)\n",
            "Collecting pyppmd<1.1.0,>=0.18.1\n",
            "  Downloading pyppmd-1.0.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.7/138.7 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyzstd>=0.14.4\n",
            "  Downloading pyzstd-0.15.7-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (399 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m399.3/399.3 kB\u001b[0m \u001b[31m49.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch>=1.3->-r /content/sp_colab/requirements.txt (line 9)) (1.11.1)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch>=1.3->-r /content/sp_colab/requirements.txt (line 9)) (2.0.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch>=1.3->-r /content/sp_colab/requirements.txt (line 9)) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch>=1.3->-r /content/sp_colab/requirements.txt (line 9)) (3.1.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.3->-r /content/sp_colab/requirements.txt (line 9)) (16.0.1)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.3->-r /content/sp_colab/requirements.txt (line 9)) (3.25.2)\n",
            "Collecting rapidfuzz==2.13.7\n",
            "  Downloading rapidfuzz-2.13.7-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m85.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=1.8.0->-r /content/sp_colab/requirements.txt (line 3)) (23.1.0)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets>=1.8.0->-r /content/sp_colab/requirements.txt (line 3)) (2.0.12)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.9.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (269 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m269.3/269.3 kB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers->-r /content/sp_colab/requirements.txt (line 1)) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers->-r /content/sp_colab/requirements.txt (line 1)) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers->-r /content/sp_colab/requirements.txt (line 1)) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch>=1.3->-r /content/sp_colab/requirements.txt (line 9)) (2.1.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets>=1.8.0->-r /content/sp_colab/requirements.txt (line 3)) (2022.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets>=1.8.0->-r /content/sp_colab/requirements.txt (line 3)) (2.8.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch>=1.3->-r /content/sp_colab/requirements.txt (line 9)) (1.3.0)\n",
            "Building wheels for collected packages: rouge-score\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24954 sha256=8787e2c67425ce5469cb4c755fb65758bff92f5622184dc472be59cf7e9fe147\n",
            "  Stored in directory: /root/.cache/pip/wheels/9b/3d/39/09558097d3119ca0a4d462df68f22c6f3c1b345ac63a09b86e\n",
            "Successfully built rouge-score\n",
            "Installing collected packages: tokenizers, texttable, sentencepiece, brotli, xxhash, rapidfuzz, pyzstd, pyppmd, pycryptodomex, pybcj, multivolumefile, multidict, lightning-utilities, inflate64, frozenlist, dill, async-timeout, yarl, rouge-score, responses, py7zr, multiprocess, jiwer, huggingface-hub, aiosignal, transformers, aiohttp, datasets, evaluate, torchmetrics, pytorch-lightning\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 brotli-1.0.9 datasets-2.11.0 dill-0.3.6 evaluate-0.4.0 frozenlist-1.3.3 huggingface-hub-0.13.4 inflate64-0.3.1 jiwer-3.0.1 lightning-utilities-0.8.0 multidict-6.0.4 multiprocess-0.70.14 multivolumefile-0.2.3 py7zr-0.20.5 pybcj-1.0.1 pycryptodomex-3.17 pyppmd-1.0.0 pytorch-lightning-2.0.1.post0 pyzstd-0.15.7 rapidfuzz-2.13.7 responses-0.18.0 rouge-score-0.1.2 sentencepiece-0.1.98 texttable-1.6.7 tokenizers-0.13.3 torchmetrics-0.11.4 transformers-4.28.1 xxhash-3.2.0 yarl-1.9.1\n"
          ]
        }
      ],
      "source": [
        "!pip3 install -r /content/sp_colab/requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ri0RVsKd7UC"
      },
      "source": [
        "## generate dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94HIg-GiC-Qf",
        "outputId": "eb6b113d-0e5b-4da0-c3d2-36506d0e0e1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading (…)lve/main/config.json: 100% 1.72k/1.72k [00:00<00:00, 285kB/s]\n",
            "Downloading (…)olve/main/vocab.json: 100% 899k/899k [00:00<00:00, 13.0MB/s]\n",
            "Downloading (…)olve/main/merges.txt: 100% 456k/456k [00:00<00:00, 720kB/s]\n",
            "Downloading (…)/main/tokenizer.json: 100% 1.36M/1.36M [00:00<00:00, 3.23MB/s]\n",
            " 33% 43173/129604 [00:36<01:12, 1192.41it/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/sp_colab/generate_dataset_gtc.py\", line 128, in <module>\n",
            "    if tokenizer_check_if_text_too_long(line, tokenizer, max_length=1024):\n",
            "  File \"/content/sp_colab/generate_dataset_gtc.py\", line 11, in tokenizer_check_if_text_too_long\n",
            "    data = tokenizer.batch_encode_plus([text], max_length=max_length, truncation=True, return_overflowing_tokens=True)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py\", line 2815, in batch_encode_plus\n",
            "    return self._batch_encode_plus(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/models/bart/tokenization_bart_fast.py\", line 261, in _batch_encode_plus\n",
            "    return super()._batch_encode_plus(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_fast.py\", line 428, in _batch_encode_plus\n",
            "    encodings = self._tokenizer.encode_batch(\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!python3 /content/sp_colab/generate_dataset_gtc.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNGgTS01unyr"
      },
      "source": [
        "## train the model with github-typo-corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kRRcHJPfDOtU",
        "outputId": "28284b57-483e-4856-9536-8f4daef77d2e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-23 22:39:21.434547: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "04/23/2023 22:39:23 - WARNING - __main__ - Process rank: -1, device: cpu, n_gpu: 0distributed training: False, 16-bits training: False\n",
            "Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-224c9b825b6d0e36/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1...\n",
            "Downloading data files: 100% 2/2 [00:00<00:00, 6136.51it/s]\n",
            "Extracting data files: 100% 2/2 [00:00<00:00, 1366.45it/s]\n",
            "Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-224c9b825b6d0e36/0.0.0/6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1. Subsequent calls will reuse this data.\n",
            "100% 2/2 [00:00<00:00, 86.83it/s]\n",
            "Downloading pytorch_model.bin: 100% 558M/558M [00:05<00:00, 101MB/s]\n",
            "Running tokenizer on train dataset:   0% 0/127601 [00:00<?, ? examples/s]/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py:3596: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/sp_colab/run_summarization.py\", line 708, in <module>\n",
            "    main()\n",
            "  File \"/content/sp_colab/run_summarization.py\", line 519, in main\n",
            "    train_dataset = train_dataset.map(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/datasets/arrow_dataset.py\", line 563, in wrapper\n",
            "    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/datasets/arrow_dataset.py\", line 528, in wrapper\n",
            "    out: Union[\"Dataset\", \"DatasetDict\"] = func(self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/datasets/arrow_dataset.py\", line 3004, in map\n",
            "    for rank, done, content in Dataset._map_single(**dataset_kwargs):\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/datasets/arrow_dataset.py\", line 3380, in _map_single\n",
            "    batch = apply_function_on_filtered_inputs(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/datasets/arrow_dataset.py\", line 3261, in apply_function_on_filtered_inputs\n",
            "    processed_inputs = function(*fn_args, *additional_args, **fn_kwargs)\n",
            "  File \"/content/sp_colab/run_summarization.py\", line 495, in preprocess_function\n",
            "    model_inputs = tokenizer(inputs, max_length=data_args.max_source_length, padding=padding, truncation=True)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py\", line 2538, in __call__\n",
            "    encodings = self._call_one(text=text, text_pair=text_pair, **all_kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py\", line 2624, in _call_one\n",
            "    return self.batch_encode_plus(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_base.py\", line 2815, in batch_encode_plus\n",
            "    return self._batch_encode_plus(\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/models/bart/tokenization_bart_fast.py\", line 261, in _batch_encode_plus\n",
            "    return super()._batch_encode_plus(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.9/dist-packages/transformers/tokenization_utils_fast.py\", line 428, in _batch_encode_plus\n",
            "    encodings = self._tokenizer.encode_batch(\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!sh /content/sp_colab/train_bart_model_gtc.sh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCdNbFVIlJW0"
      },
      "source": [
        "## ziping the model for download\n",
        "\n",
        "run this if you want to download the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SaYryhKl0anE",
        "outputId": "63d2070b-e121-4e6e-eced-be088e2cbcbb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  adding: content/sp_colab/models/ (stored 0%)\n",
            "  adding: content/sp_colab/models/.DS_Store (deflated 93%)\n",
            "  adding: content/sp_colab/models/bart-base-gtc/ (stored 0%)\n",
            "  adding: content/sp_colab/models/bart-base-gtc/training_args.bin (deflated 49%)\n",
            "  adding: content/sp_colab/models/bart-base-gtc/trainer_state.json (deflated 84%)\n",
            "  adding: content/sp_colab/models/bart-base-gtc/runs/ (stored 0%)\n",
            "  adding: content/sp_colab/models/bart-base-gtc/runs/Apr16_23-51-57_82874904c2c1/ (stored 0%)\n",
            "  adding: content/sp_colab/models/bart-base-gtc/runs/Apr16_23-51-57_82874904c2c1/1681689227.6885097/ (stored 0%)\n",
            "  adding: content/sp_colab/models/bart-base-gtc/runs/Apr16_23-51-57_82874904c2c1/1681689227.6885097/events.out.tfevents.1681689227.82874904c2c1.6170.1 (deflated 63%)\n",
            "  adding: content/sp_colab/models/bart-base-gtc/runs/Apr16_23-51-57_82874904c2c1/events.out.tfevents.1681689227.82874904c2c1.6170.0 (deflated 67%)\n",
            "  adding: content/sp_colab/models/bart-base-gtc/.DS_Store (deflated 95%)\n",
            "  adding: content/sp_colab/models/bart-base-gtc/checkpoint-500/ (stored 0%)\n",
            "  adding: content/sp_colab/models/bart-base-gtc/checkpoint-500/training_args.bin (deflated 49%)\n",
            "  adding: content/sp_colab/models/bart-base-gtc/checkpoint-500/trainer_state.json (deflated 81%)\n",
            "  adding: content/sp_colab/models/bart-base-gtc/checkpoint-500/scheduler.pt (deflated 49%)\n",
            "  adding: content/sp_colab/models/bart-base-gtc/checkpoint-500/optimizer.pt (deflated 8%)\n",
            "  adding: content/sp_colab/models/bart-base-gtc/checkpoint-500/merges.txt (deflated 53%)\n",
            "  adding: content/sp_colab/models/bart-base-gtc/checkpoint-500/special_tokens_map.json (deflated 52%)\n",
            "  adding: content/sp_colab/models/bart-base-gtc/checkpoint-500/tokenizer_config.json (deflated 50%)\n",
            "  adding: content/sp_colab/models/bart-base-gtc/checkpoint-500/config.json (deflated 64%)\n",
            "  adding: content/sp_colab/models/bart-base-gtc/checkpoint-500/pytorch_model.bin (deflated 7%)\n",
            "  adding: content/sp_colab/models/bart-base-gtc/checkpoint-500/generation_config.json (deflated 45%)\n",
            "  adding: content/sp_colab/models/bart-base-gtc/checkpoint-500/rng_state.pth (deflated 28%)\n",
            "  adding: content/sp_colab/models/bart-base-gtc/checkpoint-500/tokenizer.json (deflated 72%)\n",
            "  adding: content/sp_colab/models/bart-base-gtc/checkpoint-500/vocab.json (deflated 59%)\n",
            "  adding: content/sp_colab/models/bart-base-gtc/checkpoint-500/scaler.pt (deflated 55%)\n",
            "  adding: content/sp_colab/models/bart-base-gtc/merges.txt (deflated 53%)\n",
            "  adding: content/sp_colab/models/bart-base-gtc/special_tokens_map.json (deflated 52%)\n",
            "  adding: content/sp_colab/models/bart-base-gtc/tokenizer_config.json (deflated 50%)\n",
            "  adding: content/sp_colab/models/bart-base-gtc/config.json (deflated 64%)\n",
            "  adding: content/sp_colab/models/bart-base-gtc/checkpoint-1000/ (stored 0%)\n",
            "  adding: content/sp_colab/models/bart-base-gtc/checkpoint-1000/training_args.bin (deflated 49%)\n",
            "  adding: content/sp_colab/models/bart-base-gtc/checkpoint-1000/trainer_state.json (deflated 84%)\n",
            "  adding: content/sp_colab/models/bart-base-gtc/checkpoint-1000/scheduler.pt (deflated 49%)\n",
            "  adding: content/sp_colab/models/bart-base-gtc/checkpoint-1000/optimizer.pt (deflated 8%)\n",
            "  adding: content/sp_colab/models/bart-base-gtc/checkpoint-1000/merges.txt (deflated 53%)\n",
            "  adding: content/sp_colab/models/bart-base-gtc/checkpoint-1000/special_tokens_map.json (deflated 52%)\n",
            "  adding: content/sp_colab/models/bart-base-gtc/checkpoint-1000/tokenizer_config.json (deflated 50%)\n",
            "  adding: content/sp_colab/models/bart-base-gtc/checkpoint-1000/config.json (deflated 64%)\n",
            "  adding: content/sp_colab/models/bart-base-gtc/checkpoint-1000/pytorch_model.bin (deflated 8%)\n",
            "  adding: content/sp_colab/models/bart-base-gtc/checkpoint-1000/generation_config.json (deflated 45%)\n",
            "  adding: content/sp_colab/models/bart-base-gtc/checkpoint-1000/rng_state.pth (deflated 28%)\n",
            "  adding: content/sp_colab/models/bart-base-gtc/checkpoint-1000/tokenizer.json (deflated 72%)\n",
            "  adding: content/sp_colab/models/bart-base-gtc/checkpoint-1000/vocab.json (deflated 59%)\n",
            "  adding: content/sp_colab/models/bart-base-gtc/checkpoint-1000/scaler.pt (deflated 55%)\n",
            "  adding: content/sp_colab/models/bart-base-gtc/train_results.json (deflated 40%)\n",
            "  adding: content/sp_colab/models/bart-base-gtc/all_results.json (deflated 40%)\n",
            "  adding: content/sp_colab/models/bart-base-gtc/pytorch_model.bin (deflated 8%)\n",
            "  adding: content/sp_colab/models/bart-base-gtc/generation_config.json (deflated 45%)\n",
            "  adding: content/sp_colab/models/bart-base-gtc/tokenizer.json (deflated 72%)\n",
            "  adding: content/sp_colab/models/bart-base-gtc/vocab.json (deflated 59%)\n"
          ]
        }
      ],
      "source": [
        "!zip -r /content/models.zip /content/sp_colab/models\n",
        "from google.colab import files\n",
        "files.download(\"/content/models.zip\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1kQzKFYX8hmz",
        "outputId": "e9add13b-8cc5-4735-a498-4a40a7f1aeca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-24 00:53:16.953956: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "  1% 10/1269 [00:01<04:33,  4.60it/s]/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py:1070: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "  4% 51/1269 [00:09<03:47,  5.36it/s]\n"
          ]
        }
      ],
      "source": [
        "!python3 /content/sp_colab/spell_correction_gtc.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWGxtDE_L-8W",
        "outputId": "beb3c264-98d3-45ef-91e2-536a4dcc37a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-24 01:00:17.824320: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Downloading (…)lve/main/config.json: 100% 1.74k/1.74k [00:00<00:00, 239kB/s]\n",
            "Downloading pytorch_model.bin: 100% 558M/558M [00:05<00:00, 96.0MB/s]\n",
            "Downloading (…)okenizer_config.json: 100% 353/353 [00:00<00:00, 78.9kB/s]\n",
            "Downloading (…)olve/main/vocab.json: 100% 798k/798k [00:00<00:00, 958kB/s]\n",
            "Downloading (…)olve/main/merges.txt: 100% 456k/456k [00:00<00:00, 15.9MB/s]\n",
            "Downloading (…)/main/tokenizer.json: 100% 2.11M/2.11M [00:01<00:00, 2.01MB/s]\n",
            "Downloading (…)cial_tokens_map.json: 100% 239/239 [00:00<00:00, 74.9kB/s]\n",
            "  4% 51/1269 [01:06<26:37,  1.31s/it]\n"
          ]
        }
      ],
      "source": [
        "!python3 /content/sp_colab/spell_correction_pretrained.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jEfRP0Qi3Waq",
        "outputId": "77cb3f57-be05-4086-b561-75303efd6560"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-24 01:33:33.261950: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        }
      ],
      "source": [
        "!python3 /content/sp_colab/spell_correction_spg.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "ugv1jEMC0UnT"
      },
      "outputs": [],
      "source": [
        "!python3 /content/sp_colab/gtc_combine_predicted.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YorQb2cK2d9p"
      },
      "source": [
        "## download en_core_web_lg for creating similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5SSap7Yt2ICy",
        "outputId": "163b9aa1-adbc-49f9-ab2b-0dd573d590b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-24 01:20:06.342285: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-04-24 01:20:07.403630: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-04-24 01:20:08.793490: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-04-24 01:20:08.794012: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-04-24 01:20:08.794202: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-lg==3.5.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.5.0/en_core_web_lg-3.5.0-py3-none-any.whl (587.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.7/587.7 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /usr/local/lib/python3.9/dist-packages (from en-core-web-lg==3.5.0) (3.5.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (6.3.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.0.7)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (0.7.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (4.65.0)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (8.1.9)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.4.6)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.0.8)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.27.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (23.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.10.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (3.0.8)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (3.1.2)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (0.10.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (3.3.0)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.1.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.22.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.0.9)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.0.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (3.0.12)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (67.6.1)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.9/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2022.12.7)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.9/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.9/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (0.0.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.9/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.1.2)\n",
            "Installing collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-3.5.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_lg')\n"
          ]
        }
      ],
      "source": [
        "!python3 -m spacy download en_core_web_lg"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-iU-jfex1Urm",
        "outputId": "41384fa2-87f0-4da2-9655-603df1b136f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-04-24 02:08:40.994010: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-04-24 02:08:42.425286: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-04-24 02:08:44.329617: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-04-24 02:08:44.330270: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-04-24 02:08:44.330615: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "49 / 50 [ 2084.4991 itr/s ]\n",
            "\n",
            "___ written /content/sp_colab/data/csv/spg-small-similarity.csv ___\n",
            "time taken: 0:00:01.371000\n",
            "\n",
            "/content/sp_colab/create_similarity_csv.py:104: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
            "  lines['c-i-sim-spacy'][i] = cor_vec_spc.similarity(inc_vec_spc)\n",
            "/content/sp_colab/create_similarity_csv.py:105: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
            "  lines['c-p-sim-spacy'][i] = cor_vec_spc.similarity(pre_vec_spc)\n",
            "/content/sp_colab/create_similarity_csv.py:106: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
            "  lines['i-p-sim-spacy'][i] = inc_vec_spc.similarity(pre_vec_spc)\n",
            "50 / 51 [ 1936.7861 itr/s ]\n",
            "\n",
            "___ written /content/sp_colab/data/csv/gtc-small-similarity.csv ___\n",
            "time taken: 0:00:01.131402\n",
            "\n",
            "/content/sp_colab/create_similarity_csv.py:104: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
            "  lines['c-i-sim-spacy'][i] = cor_vec_spc.similarity(inc_vec_spc)\n",
            "/content/sp_colab/create_similarity_csv.py:106: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
            "  lines['i-p-sim-spacy'][i] = inc_vec_spc.similarity(pre_vec_spc)\n",
            "/content/sp_colab/create_similarity_csv.py:105: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
            "  lines['c-p-sim-spacy'][i] = cor_vec_spc.similarity(pre_vec_spc)\n",
            "50 / 51 [ 2819.2832 itr/s ]\n",
            "\n",
            "___ written /content/sp_colab/data/csv/gtc-small-custom-similarity.csv ___\n",
            "time taken: 0:00:01.023286\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!python3 /content/sp_colab/create_similarity_csv.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OGBjODGvD5Ir",
        "outputId": "b26a0a50-b5d0-4373-e3ed-9b0fe70af2e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Figure(2048x1024)\n"
          ]
        }
      ],
      "source": [
        "!python3 /content/sp_colab/stats.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "rRSc_jBvcOpg",
        "outputId": "1fdb4f78-61f6-4843-f11f-0a347e1e71b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  adding: content/sp_colab/data/predicted/ (stored 0%)\n",
            "  adding: content/sp_colab/data/predicted/pretrained-predicted-small-50.txt (deflated 43%)\n",
            "  adding: content/sp_colab/data/predicted/gtc-predicted-small-50.txt (deflated 46%)\n",
            "  adding: content/sp_colab/data/predicted/pretrained-predicted-small-25.txt (deflated 48%)\n",
            "  adding: content/sp_colab/data/predicted/gtc-predicted-small-25.txt (deflated 50%)\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_96dd04d0-9574-435f-9c6c-1558f13a5047\", \"predicted.zip\", 4119)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!zip -r /content/predicted.zip /content/sp_colab/data/predicted/\n",
        "from google.colab import files\n",
        "files.download(\"/content/predicted.zip\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
